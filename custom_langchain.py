# -*- coding: utf-8 -*-
"""custom_langchain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uDda5P7SBKu3pHAz_TuHeRLxXWDVmdrE

## Installation
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q transformers
!pip install -q -U sentencepiece
!pip install -q -U bitsandbytes
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git
!pip install -q langchain
!pip install -q faiss-cpu
!pip install -q scann

!tar -xvf "/content/drive/MyDrive/Colab Notebooks/projects/Private/research/개인연구_NLP/LangChain/custom_langchain/dataset/data_train.tar"
!tar -xvf "/content/drive/MyDrive/Colab Notebooks/projects/Private/research/개인연구_NLP/LangChain/custom_langchain/dataset/label_train.tar"
!tar -xvf "/content/drive/MyDrive/Colab Notebooks/projects/Private/research/개인연구_NLP/LangChain/custom_langchain/dataset/data_valid.tar"
!tar -xvf "/content/drive/MyDrive/Colab Notebooks/projects/Private/research/개인연구_NLP/LangChain/custom_langchain/dataset/label_valid.tar"

!unzip -qq "/content/019.법률,_규정_(판결서,_약관_등)_텍스트_분석_데이터/01.데이터/1.Training/라벨링데이터_230510_add/TL_1.판결문.zip.part0"
!unzip -qq "/content/019.법률,_규정_(판결서,_약관_등)_텍스트_분석_데이터/01.데이터/2.Validation/라벨링데이터_230510_add/VL_1.판결문.zip.part0"
!unzip -qq "/content/019.법률,_규정_(판결서,_약관_등)_텍스트_분석_데이터/01.데이터/1.Training/원천데이터_230510_add/TS_1.판결문.zip.part0"
!unzip -qq "/content/019.법률,_규정_(판결서,_약관_등)_텍스트_분석_데이터/01.데이터/2.Validation/원천데이터_230510_add/VS_1.판결문.zip.part0"

"""## Setup"""

# Commented out IPython magic to ensure Python compatibility.
GLOBAL_SEED = 42

import os
os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)
import sys

import random as rnd
import pandas as pd
import numpy as np
from numpy import random as np_rnd
import glob
import json
from tqdm import tqdm
import gc
import time

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, TensorDataset, DataLoader
import transformers
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig

from torch.utils.data import TensorDataset
from langchain.text_splitter import RecursiveCharacterTextSplitter
import faiss
import scann

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
# %env TOKENIZERS_PARALLELISM=true

def seed_everything(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    # python random
    rnd.seed(seed)
    # numpy random
    np_rnd.seed(seed)
    # RAPIDS random
    try:
        cupy.random.seed(seed)
    except:
        pass
    # tf random
    try:
        tf_rnd.set_seed(seed)
    except:
        pass
    # pytorch random
    try:
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    except:
        pass

def pickleIO(obj, src, op="r"):
    if op=="w":
        with open(src, op + "b") as f:
            pickle.dump(obj, f)
    elif op=="r":
        with open(src, op + "b") as f:
            tmp = pickle.load(f)
        return tmp
    else:
        print("unknown operation")
        return obj

def createFolder(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSError:
        print('Error: Creating directory. ' + directory)

def findIdx(data_x, col_names):
    return [int(i) for i, j in enumerate(data_x) if j in col_names]

def diff(first, second):
    second = set(second)
    return [item for item in first if item not in second]

def get_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    return {"all_param": all_params, "trainable_params": trainable_params, "trainable%": 100 * trainable_params / all_params}

"""## Loading rawdata"""

df = []
# corpus_container = []
for idx, fpath in enumerate(tqdm(glob.glob('/content/TL_1.판결문/**/*.json', recursive=True))):
    row = {
        # "doc_id": idx,
        "fpath": fpath,
    }
    with open(fpath, "r") as f:
        data = json.load(f)
    features_instruct = [
        "취지,원고주장,피고주장",
        "기초사실",
        "재판부의판단,재판부의결론",
    ]
    features_text = [
        # 취지, 원고 및 피고 주장
        data["mentionedItems"]["rqestObjet"] + data["assrs"]["acusrAssrs"] + data["assrs"]["dedatAssrs"],
        # 기초사실
        data["facts"]["bsisFacts"],
        # 재판부의 판단 및 결론
        data["close"]["cnclsns"] + data["dcss"]["courtDcss"],
    ]
    for feature_idx, (feature_instruct, feature_text) in enumerate(zip(features_instruct, features_text)):
        feature_text = " ".join(" ".join(feature_text).split())
        row[f"feature_{feature_instruct}"] = feature_text
    df.append(row)
    # break

df = pd.DataFrame(df)
df = df.sample(100, random_state=GLOBAL_SEED).reset_index(drop=True)
df

"""## Create Vector Data"""

class VectorDataContainer():
    def __init__(self, text_splitter=None):
        self.df_doc = None
        self.df_doc_feature = None
        if text_splitter is None:
            self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
        else:
            self.text_splitter = text_splitter

    def text_preprocess(self, text):
        return " ".join(text.split())

    def get_vectordata(self, doc_id, doc_features):
        self.df_doc = pd.DataFrame(doc_features)
        self.df_doc.index = doc_id
        df_doc_feature = []
        for idx, row in tqdm(self.df_doc.iterrows(), total=len(self.df_doc)):
            for feature_name, feature_text in row.items():
                feature_text = self.text_preprocess(feature_text)
                for chunk_id, chunk in enumerate(self.text_splitter.split_text(feature_text)):
                    df_doc_feature.append({
                        "doc_id": idx,
                        "feature_name": feature_name,
                        "chunk_id": chunk_id,
                        "chunk": f"{feature_name}: {chunk}",
                    })
        self.df_doc_feature = pd.DataFrame(df_doc_feature)

    def get_df_doc(self):
        return self.df_doc

    def get_df_doc_feature(self):
        return self.df_doc_feature

    def get_chunks(self):
        return self.df_doc_feature["chunk"].to_list()

vector_data = VectorDataContainer(text_splitter=RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20))
vector_data.get_vectordata(
    doc_id=range(len(df)),
    doc_features={
        "기초사실": df["feature_기초사실"],
    }
)

"""## Create Vector Embedding"""

class MeanPooling(nn.Module):
    def __init__(self):
        super(MeanPooling, self).__init__()

    def forward(self, last_hidden_state, attention_mask):
        # expand attention mask to embedding dimension for element-wise multiplication
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        # element-wise multiplication & summation by embedding dim (if attention mask is zero, not pooled in final embedding vector)
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)
        # get the number of attention mask by each dims
        sum_mask = input_mask_expanded.sum(1)
        # clip to small number for prevent from div 0
        sum_mask = torch.clamp(sum_mask, min=1e-9)
        # get final mean pooled embedding vector
        mean_embeddings = sum_embeddings / sum_mask
        return mean_embeddings

class VectorEmbedding():
    def __init__(self, model_id, max_length=512):
        self.vectors = None
        self.store = None
        self.model_id = model_id
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        self.max_length = max_length
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.tokenizer_params = {
            "max_length": max_length,
            "padding": "max_length",
            "truncation": True,
            "return_attention_mask": True,
            "return_token_type_ids": False,
            "return_tensors": "pt"
        }
        self.model = AutoModel.from_pretrained(model_id)
        self.model.to(self.device)
        self.model.eval()

    def tokenize(self, x):
        tokens = self.tokenizer.batch_encode_plus(
            x, **self.tokenizer_params,
        )
        return tokens

    def get_vectorembedding(self, docs, batch_size=128, norm=True):
        embed = []
        pooler = MeanPooling()
        tokens = self.tokenize([docs] if isinstance(docs, str) else docs)
        dl = DataLoader(TensorDataset(tokens["input_ids"], tokens["attention_mask"]), batch_size=batch_size, shuffle=False)
        with torch.no_grad():
            for idx, batch in enumerate(tqdm(dl)):
                # input_ids
                batch[0] = batch[0].to(self.device)
                # attention_mask
                batch[1] = batch[1].to(self.device)
                output = self.model(**{"input_ids": batch[0], "attention_mask": batch[1]})
                embed.append(pooler(output.last_hidden_state, batch[1]))
                del batch, output
                torch.cuda.empty_cache()
                gc.collect()
        del pooler, tokens, dl
        torch.cuda.empty_cache()
        gc.collect()
        embed = torch.cat(embed, dim=0).to(torch.float32)
        return F.normalize(embed, p=2, dim=1).detach().cpu().numpy() if norm else embed.detach().cpu().numpy()

# create vector embedding class
model_id = "microsoft/Multilingual-MiniLM-L12-H384"
model_seq_len = 512
vector_embedding = VectorEmbedding(model_id, max_length=model_seq_len)
embedding = vector_embedding.get_vectorembedding(vector_data.get_chunks())

"""## Create Vector Store & Ranker"""

class FaissVectorStore():
    def __init__(self, vector_data, ranker, similarity_algorithm="dot_product", k=100_000):
        self.corpus_container = vector_data.get_df_doc_feature()[["doc_id"]]
        self.corpus_container["scores"] = -1.0
        self.corpus_container["scores"] = self.corpus_container["scores"].astype("float32")
        self.store = None
        self.ranker = ranker
        if similarity_algorithm not in ["dot_product"]:
            ValueError(f"{self.similarity_algorithm} is not supported.")
        else:
            self.similarity_algorithm = similarity_algorithm
        self.k = k
        self.max_gpu_k = 2048

    def get_vectorstore(self, embedding, use_gpu=False):
        if self.similarity_algorithm == "dot_product":
            self.store = faiss.IndexFlatIP(embedding.shape[-1])
        else:
            ValueError(f"{self.similarity_algorithm} is not supported.")
        if use_gpu:
            self.store = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, self.store)
        self.store.add(embedding)

    def search(self, embedding, use_gpu=False):
        # transform (E) -> (1, E)
        if len(embedding.shape) == 1:
            embedding = embedding.reshape(1, -1)
        scores, indicies = self.store.search(embedding, k=min(self.max_gpu_k, self.k) if use_gpu else self.k)
        # normalize 0-1
        if self.similarity_algorithm == "dot_product":
            scores = ((scores + 1.0) / 2.0)
        # ranking
        return self.ranker(self.corpus_container, scores, indicies)


class ScannVectorStore():
    def __init__(self, vector_data, ranker, similarity_algorithm="dot_product", k=100_000):
        self.corpus_container = vector_data.get_df_doc_feature()[["doc_id"]]
        self.corpus_container["scores"] = -1.0
        self.corpus_container["scores"] = self.corpus_container["scores"].astype("float32")
        self.store = None
        self.ranker = ranker
        if similarity_algorithm not in ["dot_product"]:
            ValueError(f"{self.similarity_algorithm} is not supported.")
        else:
            self.similarity_algorithm = similarity_algorithm
        self.k = k

    def get_vectorstore(self, embedding, building_params={"build": {"num_leaves": 2000, "num_leaves_to_search": 100, "training_sample_size": 250000}}):
        if self.similarity_algorithm == "dot_product":
            self.store = scann.scann_ops_pybind.builder(embedding, num_neighbors=self.k, "dot_product")
        else:
            ValueError(f"{self.similarity_algorithm} is not supported.")
        self.store = self.store.tree(**building_params["build"])
        self.store = self.store.score_ah(dimensions_per_block=2, anisotropic_quantization_threshold=0.2)
        self.store = self.store.reorder(reordering_num_neighbors=100)
        self.store = self.store.build()

    def search(self, embedding, searching_params={"leaves_to_search": 150, "pre_reorder_num_neighbors": 250}):
        # transform (E) -> (1, E)
        if len(embedding.shape) == 1:
            embedding = embedding.reshape(1, -1)
        indicies, scores = self.store.search_batched(embedding, **searching_params)
        # normalize 0-1
        if self.similarity_algorithm == "dot_product":
            scores = ((scores + 1.0) / 2.0)
        # ranking
        return self.ranker(self.corpus_container, scores, indicies)

class VectorRanker():
    def __init__(self, topN_chunks=10_000, n_retrievals=10, ranking_type="first_matching"):
        self.topN_chunks = topN_chunks
        self.n_retrievals = n_retrievals
        if ranking_type not in ['first_matching', 'equal_weighted', 'exponential_weighted']:
            raise ValueError("ranking_type must be in ['first_matching', 'equal_weighted', 'exponential_weighted']")
        else:
            self.ranking_type = ranking_type

    def __call__(self, corpus_container, scores, indicies):
        corpus_container["scores"] = -1.0
        # get averaged score on each documents
        # average score by equal weight and get Top N docs
        if self.ranking_type == "equal_weighted":
            # assign score
            corpus_container["scores"].iloc[indicies[0]] = scores[0]
            # get only topN chunks
            candidates = corpus_container.sort_values("scores", ascending=False).iloc[:self.topN_chunks]
            retrieval_docs = candidates.groupby("doc_id", sort=False, as_index=False)["scores"].apply(lambda x: x.mean()).sort_values("scores", ascending=False)
        # average score by exponential weight and get Top N docs
        elif self.ranking_type == "exponential_weighted":
            # assign score
            corpus_container["scores"].iloc[indicies[0]] = scores[0] * (np.logspace(start=0, stop=1, num=len(scores[0])) / 10.0)
            # get only topN chunks
            candidates = corpus_container.sort_values("scores", ascending=False).iloc[:self.topN_chunks]
            retrieval_docs = candidates.groupby("doc_id", sort=False, as_index=False)["scores"].apply(lambda x: x.mean()).sort_values("scores", ascending=False)
        # first matched N docs
        else:
            # assign score
            corpus_container["scores"].iloc[indicies[0]] = scores[0]
            # get only topN chunks
            candidates = corpus_container.sort_values("scores", ascending=False).iloc[:self.topN_chunks]
            retrieval_docs = candidates.drop_duplicates(subset="doc_id")
        retrieval_docs = retrieval_docs["doc_id"].iloc[:self.n_retrievals]
        return pd.concat([corpus_container[corpus_container["doc_id"] == doc_id].sort_values("scores", ascending=False) for doc_id in retrieval_docs.values], axis=0).reset_index(drop=True)

# create vector store class
# for faiss
vector_ranker = VectorRanker(topN_chunks=10_000, n_retrievals=10, ranking_type="exponential_weighted")
vector_store = FaissVectorStore(vector_data, vector_ranker)
vector_store.get_vectorstore(embedding, use_gpu=False)
# # for scann
# scann_params = {
#     "build": {
#         "num_leaves": 100,
#         "num_leaves_to_search": 10,
#         "training_sample_size": int(len(embedding) * 0.2),
#     },
# }
# vector_store = VectorStore(vector_data, store_model_type="scann")
# vector_store.get_vectorstore(embedding, building_params=scann_params)

# example
question = "부동산 취득세 관련 판결"
question_embedding = vector_embedding.get_vectorembedding(question)
output = vector_store.search(question_embedding)
output

"""## Generation with Retrieval Documents"""

class HuggingFaceAPI():
    def __init__(self, model_id, vector_embedding, vector_store, generate_params, quantization_params=None, max_length=512):
        self.vector_embedding = vector_embedding
        self.vector_store = vector_store
        self.model_id = model_id
        if generate_params is True:
            generate_params = {
                "max_length": 1000,
                "num_beams": 3,
                "do_sample": True,
                "temperature": 0.9,
                "top_k": 50,
                "top_p": 0.95,
                "repetition_penalty": 1.2,
                "length_penalty": 1.0,
                "eos_token_id": 2,
            }
            generate_params["early_stopping"] = True if generate_params["num_beams"] > 1 else False
        self.generate_params = generate_params
        if quantization_params is True:
            quantization_params = BitsAndBytesConfig(
                # 모델을 4bit로 로딩하도록 설정합니다
                load_in_4bit=True,
                # double quantization 모드를 활성화합니다 (weight 저장과 계산을 다른 타입으로 할 수 있게 합니다)
                bnb_4bit_use_double_quant=True,
                # double quantization 모드에서 저장될 데이터 타입을 지정합니다
                bnb_4bit_quant_type="nf4",
                # double quantization 모드에서 계산에 데이터 타입을 지정합니다
                bnb_4bit_compute_dtype=torch.bfloat16,
                # set device
                device_map="auto",
            )
        self.quantization_params = quantization_params
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        self.max_length = max_length
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side="left")
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        self.tokenizer_params = {
            "max_length": max_length,
            "padding": "max_length",
            "truncation": True,
            "return_attention_mask": True,
            "return_token_type_ids": False,
            "return_tensors": "pt"
        }
        if quantization_params is not None:
            self.model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_params)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(model_id)
            self.model.to(self.device)
        self.model.eval()
        self.model.generation_config.pad_token_id = self.model.generation_config.eos_token_id
        self.model.config.use_cache = True

    def tokenize(self, x):
        tokens = self.tokenizer.encode_plus(
            x, **self.tokenizer_params,
        )
        return tokens

    def create_prompt_template(self, lang="kor"):
        if lang in ["kr", "kor"]:
            prompt = """지시문: 문맥을 참고하여 질문에 알맞는 답변을 해주세요. 문맥은 ```구분자 안에 있습니다. 모르는 질문이면 '잘 모르겠습니다.'라고 답변해주세요.

문맥
```
{context}
```

질문: {question}

답변: """
        else:
            prompt = """Instruction: Please reply on question referring to context. Context is in ```separator. If you don't know about question, please reply 'I don't know.'.

Context
```
{context}
```

Question: {question}

Answer: """
        return prompt

    def generate(
            self, prompt, question, doc_keyword="document", num_context_docs=1, feature_length_strategy="balanced",
            max_context_length=1000, max_feature_length=100, feature_length_threshold=80,
        ):
        # retrieval
        retrieval_docs = vector_store.search(vector_embedding.get_vectorembedding(question))
        # create context from retrieved documents
        feature_names = vector_data.get_df_doc().columns
        context = []
        if feature_length_strategy == "balanced":
            feature_lengths = np.array([np.percentile(vector_data.get_df_doc()[col].apply(len), feature_length_threshold) for col in feature_names])
            feature_lengths = ((feature_lengths / feature_lengths.sum()) * feature_length_threshold).astype("int32")
            for idx, doc_id in enumerate(retrieval_docs["doc_id"].iloc[:num_context_docs]):
                context.append(f"[{doc_keyword}{idx+1}]\n" + "\n".join([f"{k.split('_')[-1]}: {v[:max_len]}" for max_len, (k, v) in zip(feature_lengths, vector_data.get_df_doc().loc[doc_id].items())]))
        else:
            for idx, doc_id in enumerate(retrieval_docs["doc_id"].iloc[:num_context_docs]):
                context.append(f"[{doc_keyword}{idx+1}]\n" + "\n".join([f"{k.split('_')[-1]}: {v[:max_len]}" for max_len, (k, v) in zip([max_feature_length] * len(feature_names), vector_data.get_df_doc().loc[doc_id].items())]))
        context = "\n".join(context)
        # create prompt
        prompt = prompt.replace("{context}", context).replace("{question}", question)
        # tokenizing
        tokens = self.tokenize(prompt)
        # generate
        start_time = time.time()
        with torch.no_grad():
            for batch in DataLoader(TensorDataset(tokens["input_ids"], tokens["attention_mask"]), batch_size=1, shuffle=False):
                batch[0] = batch[0].to(device)
                batch[1] = batch[1].to(device)
                gened = llm.model.generate(
                    **{"input_ids": batch[0], "attention_mask": batch[1]},
                    **generate_params,
                )
            response = llm.tokenizer.batch_decode(gened, skip_special_tokens=True)[0]
        end_time = time.time()
        # decoding
        output = {
            "retrieval_docs": retrieval_docs,
            "response": response,
            "inference_runtime": round(end_time - start_time, 3),
        }
        return output

generate_params = {
    "max_new_tokens": 300,
    "num_beams": 3,
    "do_sample": True,
    "temperature": 0.7,
    "top_k": 50,
    "top_p": 0.9,
    "length_penalty": 0.8,
    "repetition_penalty": 1.2,
    "no_repeat_ngram_size": 3,
    "eos_token_id": 2,
}
generate_params["early_stopping"] = True if generate_params["num_beams"] > 1 else False

# config on model for quantization
quantization_params = BitsAndBytesConfig(
    # 모델을 4bit로 로딩하도록 설정합니다
    load_in_4bit=True,
    # double quantization 모드를 활성화합니다 (weight 저장과 계산을 다른 타입으로 할 수 있게 합니다)
    bnb_4bit_use_double_quant=True,
    # double quantization 모드에서 저장될 데이터 타입을 지정합니다
    bnb_4bit_quant_type="nf4",
    # double quantization 모드에서 계산에 데이터 타입을 지정합니다
    bnb_4bit_compute_dtype=torch.bfloat16,
    # set device
    device_map="auto",
)

model_id = "EleutherAI/polyglot-ko-1.3b"
llm_seq_len = 2048
llm = HuggingFaceAPI(model_id, vector_embedding, vector_store, generate_params=generate_params, quantization_params=quantization_params, max_length=llm_seq_len)

prompt = llm.create_prompt_template()
question = "금은품 절도 사건 관련한 문서의 재판부 판결을 요약해주세요."
output = llm.generate(
    prompt, question, doc_keyword="문서",
    feature_length_strategy="balanced", feature_length_threshold=80, max_context_length=int(llm_seq_len * 0.5),
)

print(output["response"])

